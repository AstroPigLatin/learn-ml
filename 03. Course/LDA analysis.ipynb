{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib2\n",
    "import datetime\n",
    "from itertools import ifilter\n",
    "from collections import Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import bibtexparser\n",
    "\n",
    "pd.set_option('mode.chained_assignment','warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OAI = \"{http://www.openarchives.org/OAI/2.0/}\"\n",
    "ARXIV = \"{http://arxiv.org/OAI/arXiv/}\"\n",
    "\n",
    "def harvest(arxiv=\"physics:hep-ex\"):\n",
    "    df = pd.DataFrame(columns=(\"title\", \"abstract\", \"categories\", \"created\", \"id\", \"doi\"))\n",
    "    base_url = \"http://export.arxiv.org/oai2?verb=ListRecords&\"\n",
    "    url = (base_url +\n",
    "           \"from=2010-01-01&until=2014-12-31&\" +\n",
    "           \"metadataPrefix=arXiv&set=%s\"%arxiv)\n",
    "    \n",
    "    while True:\n",
    "        print \"fetching\", url\n",
    "        try:\n",
    "            response = urllib2.urlopen(url)\n",
    "            \n",
    "        except urllib2.HTTPError, e:\n",
    "            if e.code == 503:\n",
    "                to = int(e.hdrs.get(\"retry-after\", 30))\n",
    "                print \"Got 503. Retrying after {0:d} seconds.\".format(to)\n",
    "\n",
    "                time.sleep(to)\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "        xml = response.read()\n",
    "\n",
    "        root = ET.fromstring(xml)\n",
    "\n",
    "        for record in root.find(OAI+'ListRecords').findall(OAI+\"record\"):\n",
    "            arxiv_id = record.find(OAI+'header').find(OAI+'identifier')\n",
    "            meta = record.find(OAI+'metadata')\n",
    "            info = meta.find(ARXIV+\"arXiv\")\n",
    "            created = info.find(ARXIV+\"created\").text\n",
    "            created = datetime.datetime.strptime(created, \"%Y-%m-%d\")\n",
    "            categories = info.find(ARXIV+\"categories\").text\n",
    "\n",
    "            # if there is more than one DOI use the first one\n",
    "            # often the second one (if it exists at all) refers\n",
    "            # to an eratum or similar\n",
    "            doi = info.find(ARXIV+\"doi\")\n",
    "            if doi is not None:\n",
    "                doi = doi.text.split()[0]\n",
    "                \n",
    "            contents = {'title': info.find(ARXIV+\"title\").text,\n",
    "                        'id': info.find(ARXIV+\"id\").text,#arxiv_id.text[4:],\n",
    "                        'abstract': info.find(ARXIV+\"abstract\").text.strip(),\n",
    "                        'created': created,\n",
    "                        'categories': categories.split(),\n",
    "                        'doi': doi,\n",
    "                        }\n",
    "\n",
    "            df = df.append(contents, ignore_index=True)\n",
    "\n",
    "        # The list of articles returned by the API comes in chunks of\n",
    "        # 1000 articles. The presence of a resumptionToken tells us that\n",
    "        # there is more to be fetched.\n",
    "        token = root.find(OAI+'ListRecords').find(OAI+\"resumptionToken\")\n",
    "        if token is None or token.text is None:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            url = base_url + \"resumptionToken=%s\"%(token.text)\n",
    "            \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&from=2010-01-01&until=2014-12-31&metadataPrefix=arXiv&set=physics:hep-ex\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|1001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|2001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|3001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|4001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|5001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|6001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|7001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|8001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|9001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|10001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|11001\n",
      "fetching http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken=2398479|12001\n"
     ]
    }
   ],
   "source": [
    "df = harvest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>created</th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Measurement of B(D_S^+ --&gt; ell^+ nu) and the D...</td>\n",
       "      <td>We examine e+e- --&gt; Ds- Ds*+ and Ds*- Ds+ inte...</td>\n",
       "      <td>[hep-ex, hep-lat, hep-ph]</td>\n",
       "      <td>2007-04-03</td>\n",
       "      <td>0704.0437</td>\n",
       "      <td>10.1103/PhysRevD.76.072002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A unified analysis of the reactor neutrino pro...</td>\n",
       "      <td>We present in this article a detailed quantita...</td>\n",
       "      <td>[hep-ex]</td>\n",
       "      <td>2007-04-04</td>\n",
       "      <td>0704.0498</td>\n",
       "      <td>10.1088/1742-6596/110/8/082013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Measurement of Decay Amplitudes of B --&gt;(c cba...</td>\n",
       "      <td>We perform the first three-dimensional measure...</td>\n",
       "      <td>[hep-ex]</td>\n",
       "      <td>2007-04-04</td>\n",
       "      <td>0704.0522</td>\n",
       "      <td>10.1103/PhysRevD.76.031102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Measurement of the Decay Constant $f_D{_S^+}$ ...</td>\n",
       "      <td>We measure the decay constant fDs using the Ds...</td>\n",
       "      <td>[hep-ex, hep-lat, hep-ph]</td>\n",
       "      <td>2007-04-04</td>\n",
       "      <td>0704.0629</td>\n",
       "      <td>10.1103/PhysRevLett.99.071802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The $e^+ e^-\\to K^+ K^- \\pi^+\\pi^-$, $K^+ K^- ...</td>\n",
       "      <td>We study the processes $e^+ e^-\\to K^+ K^- \\pi...</td>\n",
       "      <td>[hep-ex]</td>\n",
       "      <td>2007-04-04</td>\n",
       "      <td>0704.0630</td>\n",
       "      <td>10.1103/PhysRevD.76.012008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Measurement of B(D_S^+ --> ell^+ nu) and the D...   \n",
       "1  A unified analysis of the reactor neutrino pro...   \n",
       "2  Measurement of Decay Amplitudes of B -->(c cba...   \n",
       "3  Measurement of the Decay Constant $f_D{_S^+}$ ...   \n",
       "4  The $e^+ e^-\\to K^+ K^- \\pi^+\\pi^-$, $K^+ K^- ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We examine e+e- --> Ds- Ds*+ and Ds*- Ds+ inte...   \n",
       "1  We present in this article a detailed quantita...   \n",
       "2  We perform the first three-dimensional measure...   \n",
       "3  We measure the decay constant fDs using the Ds...   \n",
       "4  We study the processes $e^+ e^-\\to K^+ K^- \\pi...   \n",
       "\n",
       "                  categories    created         id  \\\n",
       "0  [hep-ex, hep-lat, hep-ph] 2007-04-03  0704.0437   \n",
       "1                   [hep-ex] 2007-04-04  0704.0498   \n",
       "2                   [hep-ex] 2007-04-04  0704.0522   \n",
       "3  [hep-ex, hep-lat, hep-ph] 2007-04-04  0704.0629   \n",
       "4                   [hep-ex] 2007-04-04  0704.0630   \n",
       "\n",
       "                              doi  \n",
       "0      10.1103/PhysRevD.76.072002  \n",
       "1  10.1088/1742-6596/110/8/082013  \n",
       "2      10.1103/PhysRevD.76.031102  \n",
       "3   10.1103/PhysRevLett.99.071802  \n",
       "4      10.1103/PhysRevD.76.012008  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12399 entries, 0 to 12398\n",
      "Data columns (total 6 columns):\n",
      "title         12399 non-null object\n",
      "abstract      12399 non-null object\n",
      "categories    12399 non-null object\n",
      "created       12399 non-null datetime64[ns]\n",
      "id            12399 non-null object\n",
      "doi           8263 non-null object\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 581.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA examples\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n"
     ]
    }
   ],
   "source": [
    "raw = doc_a.lower()\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'likes', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', u'like', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "# stem token\n",
    "stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "texts = stemmed_tokens\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for d in doc_set:\n",
    "    raw = d.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.125*\"health\" + 0.050*\"increas\" + 0.050*\"blood\"'), (1, u'0.059*\"drive\" + 0.059*\"pressur\" + 0.059*\"seem\"'), (2, u'0.082*\"good\" + 0.082*\"brocolli\" + 0.081*\"brother\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.087*\"good\" + 0.087*\"brocolli\" + 0.063*\"mother\" + 0.063*\"brother\"'), (1, u'0.067*\"drive\" + 0.066*\"pressur\" + 0.039*\"school\" + 0.039*\"seem\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the ArXiv df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = df['abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for d in abstracts:\n",
    "    raw = d.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.018*\"model\" + 0.014*\"mass\" + 0.012*\"can\" + 0.010*\"neutrino\" + 0.008*\"effect\"'), (1, u'0.024*\"neutrino\" + 0.016*\"detector\" + 0.016*\"experi\" + 0.012*\"energi\" + 0.010*\"measur\"'), (2, u'0.031*\"decay\" + 0.030*\"b\" + 0.015*\"d\" + 0.014*\"pi\" + 0.013*\"cp\"'), (3, u'0.026*\"2\" + 0.014*\"q\" + 0.014*\"gev\" + 0.013*\"p\" + 0.013*\"data\"'), (4, u'0.024*\"mass\" + 0.021*\"1\" + 0.021*\"boson\" + 0.017*\"model\" + 0.016*\"gev\"'), (5, u'0.024*\"measur\" + 0.018*\"jet\" + 0.017*\"product\" + 0.017*\"collis\" + 0.015*\"cross\"'), (6, u'0.085*\"0\" + 0.027*\"pi\" + 0.024*\"1\" + 0.023*\"2\" + 0.023*\"b\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=20, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
